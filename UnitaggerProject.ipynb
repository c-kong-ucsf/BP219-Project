{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj7xA44e9bvA"
      },
      "source": [
        "##Intro to Programming Project: Uni-Tagger\n",
        "Please run all cells in order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "iccGdbe_Pmt9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#@title 1. Install dependencies\n",
        "%%bash -s $use_amber $use_templates\n",
        "\n",
        "set -e\n",
        "\n",
        "USE_AMBER=$1\n",
        "USE_TEMPLATES=$2\n",
        "\n",
        "if [ ! -f COLABFOLD_READY ]; then\n",
        "  # install dependencies\n",
        "  # We have to use \"--no-warn-conflicts\" because colab already has a lot preinstalled with requirements different to ours\n",
        "  pip install -q --no-warn-conflicts \"colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold\"\n",
        "  # high risk high gain\n",
        "  pip install -q \"jax[cuda11_cudnn805]>=0.3.8,<0.4\" -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "  touch COLABFOLD_READY\n",
        "fi\n",
        "\n",
        "# setup conda\n",
        "if [ ${USE_AMBER} == \"True\" ] || [ ${USE_TEMPLATES} == \"True\" ]; then\n",
        "  if [ ! -f CONDA_READY ]; then\n",
        "    wget -qnc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "    bash Miniconda3-latest-Linux-x86_64.sh -bfp /usr/local 2>&1 1>/dev/null\n",
        "    rm Miniconda3-latest-Linux-x86_64.sh\n",
        "    touch CONDA_READY\n",
        "  fi\n",
        "fi\n",
        "# setup template search\n",
        "if [ ${USE_TEMPLATES} == \"True\" ] && [ ! -f HH_READY ]; then\n",
        "  conda install -y -q -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python=3.7 2>&1 1>/dev/null\n",
        "  touch HH_READY\n",
        "fi\n",
        "# setup openmm for amber refinement\n",
        "if [ ${USE_AMBER} == \"True\" ] && [ ! -f AMBER_READY ]; then\n",
        "  conda install -y -q -c conda-forge openmm=7.5.1 python=3.7 pdbfixer 2>&1 1>/dev/null\n",
        "  touch AMBER_READY\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yE0sYroghJRg",
        "cellView": "form",
        "outputId": "de79b091-96bf-4e50-a4c5-e824698ba349",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.3.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 27.1 MB/s \n",
            "\u001b[?25hCollecting paramiko\n",
            "  Downloading paramiko-2.11.0-py2.py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 68.2 MB/s \n",
            "\u001b[?25hCollecting orjson\n",
            "  Downloading orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 70.9 MB/s \n",
            "\u001b[?25hCollecting h11<0.13,>=0.11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from gradio) (6.0)\n",
            "Collecting analytics-python\n",
            "  Downloading analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.85.0-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting websockets\n",
            "  Downloading websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 75.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gradio) (3.8.1)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 58.6 MB/s \n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.28.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from gradio) (2022.8.2)\n",
            "Collecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.18.3-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.3.5)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from gradio) (1.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from gradio) (2.11.3)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (2.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.1.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (2.8.2)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (1.15.0)\n",
            "Collecting backoff==1.10.0\n",
            "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.26.12)\n",
            "Collecting starlette==0.20.4\n",
            "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 10.7 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.16.0,>=0.15.0\n",
            "  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->gradio) (2.0.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2022.2.1)\n",
            "Collecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[K     |████████████████████████████████| 856 kB 61.7 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-4.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (594 kB)\n",
            "\u001b[K     |████████████████████████████████| 594 kB 72.7 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 54.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.5->paramiko->gradio) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Building wheels for collected packages: ffmpy, python-multipart\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=57acf1cdfa058ed65085701f125f27c3d57052fbb45d4df00e8e9cd84fdab058\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=7a28f6681da3899fa6f7b6af2e70ea38ae51376248591bb9ff173c25f7c0218d\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "Successfully built ffmpy python-multipart\n",
            "Installing collected packages: sniffio, mdurl, uc-micro-py, rfc3986, markdown-it-py, h11, anyio, starlette, pynacl, monotonic, mdit-py-plugins, linkify-it-py, httpcore, cryptography, bcrypt, backoff, websockets, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, httpx, ffmpy, fastapi, analytics-python, gradio\n",
            "Successfully installed analytics-python-1.4.0 anyio-3.6.1 backoff-1.10.0 bcrypt-4.0.0 cryptography-38.0.1 fastapi-0.85.0 ffmpy-0.3.0 gradio-3.3.1 h11-0.12.0 httpcore-0.15.0 httpx-0.23.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.0 mdurl-0.1.2 monotonic-1.6 orjson-3.8.0 paramiko-2.11.0 pycryptodome-3.15.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.20.4 uc-micro-py-1.0.1 uvicorn-0.18.3 websockets-10.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Bio\n",
            "  Downloading bio-1.4.0-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 30.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Bio) (2.28.1)\n",
            "Requirement already satisfied: biopython>=1.79 in /usr/local/lib/python3.7/dist-packages (from Bio) (1.79)\n",
            "Collecting mygene\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from Bio) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython>=1.79->Bio) (1.21.6)\n",
            "Collecting biothings-client>=0.2.6\n",
            "  Downloading biothings_client-0.2.6-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (1.26.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2.10)\n",
            "Installing collected packages: biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.4.0 biothings-client-0.2.6 mygene-3.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rmsd\n",
            "  Downloading rmsd-1.4-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rmsd) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from rmsd) (1.7.3)\n",
            "Installing collected packages: rmsd\n",
            "Successfully installed rmsd-1.4\n"
          ]
        }
      ],
      "source": [
        "#@title 2. Import modules/packages\n",
        "#Loading all necessary modules/packages\n",
        "#Input protein sequence(s)\n",
        "from google.colab import files\n",
        "import os.path\n",
        "from os import path\n",
        "import re\n",
        "import hashlib\n",
        "import random\n",
        "\n",
        "#Run Prediction\n",
        "import sys\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "#User Iterface\n",
        "import sys\n",
        "!{sys.executable} -m pip install gradio\n",
        "import gradio  as gr\n",
        "\n",
        "#RMSD Calculations\n",
        "!{sys.executable} -m pip install Bio\n",
        "!{sys.executable} -m pip install rmsd\n",
        "import Bio\n",
        "from Bio import PDB\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "\n",
        "from colabfold.download import download_alphafold_params, default_data_dir\n",
        "from colabfold.utils import setup_logging\n",
        "from colabfold.batch import get_queries, run, set_model_type\n",
        "K80_chk = !nvidia-smi | grep \"Tesla K80\" | wc -l\n",
        "if \"1\" in K80_chk:\n",
        "  print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")\n",
        "  if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n",
        "    del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n",
        "  if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n",
        "    del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n",
        "\n",
        "from colabfold.colabfold import plot_protein\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Creates the directory path where the created PDB files are stored (if it doesnt exist already)\n",
        "if(not (os.path.isdir(\"PDBs\"))):\n",
        "  os.mkdir(\"PDBs\")\n",
        "\n",
        "if(not (os.path.isdir(\"aligned\"))):\n",
        "  os.mkdir(\"aligned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wAgO2BZE0Vml"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 3. Run ColabFold Functionality\n",
        "#@markdown The pdb files utilized for RMSD ranking are stored in the \"PDBs\" directory  \n",
        "\n",
        "#Functional approach to iterating over given dictionary of values\n",
        "#Dictionary for TAG sequences\n",
        "tag_sequences = {\"6X His\":\"HHHHHH\", \"CBP\":\"KRRWKKNFIAVSAANRFKKISSSGAL\",\n",
        "                     \"CBD + 3G\": \"GGGTNPGVSAWQVNTAYTAGQLVTYNGKTYKCLQPHTSLAGWEPSNVPALWQLQ\",\n",
        "                     \"mNEONGreen + 5G\": \"GGGGGMVSKGEEDNMASLPATHELHIFGSINGVDFDMVGQGTGNPNDGYEELNLKSTKGDLQFSPWILVPHIGYGFHQYLPYPDGMSPFQAAMVDGSGYQVHRTMQFEDGASLTVNYRYTYEGSHIKGEAQVKGTGFPADGPVMTNSLTAADWCRSKKTYPNDKTIISTFKWSYTTGNGKRYRSTARTTYTFAKPMAANYLKNQPMYVFRKTELKHSKTELNFKEWQKAFTDVMGMDELYK\"\n",
        "                     }\n",
        "\n",
        "#Hash function for producing keys for appending to jobname\n",
        "def add_hash(x,y):\n",
        "  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "#Takes a dictionary of values and builds an array of all the filepaths\n",
        "def input_dict(variant_dict,isAdd):\n",
        "  #Clears PDBs files for analysis, so that new inputs can added\n",
        "  PDB_dir = \"/content/PDBs/\"\n",
        "  if (isAdd):\n",
        "    for PDB in os.listdir(PDB_dir):\n",
        "      os.remove(os.path.join(PDB_dir,PDB))\n",
        "\n",
        "  paths_dict = {}\n",
        "  for name in variant_dict:\n",
        "    # Initialization of the query_sequence\n",
        "    # Removes whitespaces\n",
        "    query_sequence = variant_dict[name]\n",
        "    query_sequence = \"\".join(query_sequence.split())\n",
        "    # Initialization of the jobname\n",
        "    # Removes whitespaces and append hash key\n",
        "    jobname = name\n",
        "    basejobname = \"\".join(jobname.split())\n",
        "    basejobname = re.sub(r'\\W+', '', basejobname)\n",
        "    jobname = add_hash(basejobname, query_sequence)\n",
        "    while os.path.isfile(f\"{jobname}.csv\"):\n",
        "      jobname = add_hash(basejobname, ''.join(random.sample(query_sequence,len(query_sequence))))\n",
        "\n",
        "    with open(f\"{jobname}.csv\", \"w\") as text_file:\n",
        "      text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n",
        "\n",
        "    #local variables needed to run prediction (most of these are adjustable, but for our purposes we are running them as default)\n",
        "    #MS\n",
        "    queries_path=f\"{jobname}.csv\"\n",
        "    use_amber = False\n",
        "    template_mode = \"none\"\n",
        "    custom_template_path = None\n",
        "    use_templates = False\n",
        "    msa_mode = \"MMseqs2 (UniRef+Environmental)\"\n",
        "    pair_mode = \"unpaired+paired\"\n",
        "    a3m_file = f\"{jobname}.a3m\"\n",
        "    model_type = \"auto\"\n",
        "    num_recycles = 3\n",
        "    save_to_google_drive = False\n",
        "    dpi = 200\n",
        "\n",
        "    input_array = [queries_path,use_amber,template_mode,custom_template_path,use_templates,msa_mode,pair_mode,a3m_file,model_type,num_recycles,save_to_google_drive,dpi]\n",
        "\n",
        "    if use_amber and '/usr/local/lib/python3.7/site-packages/' not in sys.path:\n",
        "      sys.path.insert(0, '/usr/local/lib/python3.7/site-packages/')\n",
        "\n",
        "    jobname_path = run_prediction(jobname,input_array,\"PDBs\")\n",
        "    paths_dict[name] = jobname_path\n",
        "\n",
        "  #Deletes all files stored in the sample data (to clear room for subsequent runs)\n",
        "  delete_dir = \"/content/\"\n",
        "  for file in os.listdir(delete_dir):\n",
        "    if(path.isfile(file)):\n",
        "      os.remove(os.path.join(delete_dir,file))\n",
        "\n",
        "  return paths_dict\n",
        "\n",
        "# code from ColabFold to call prediction. \n",
        "def prediction_callback(unrelaxed_protein, length, prediction_result, input_features, type):\n",
        "  fig = plot_protein(unrelaxed_protein, Ls=length, dpi=150)\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "#Runs a prediction and moves the resulting rank 1 pdb structure (model does not matter) into a desired directory\n",
        "#Must provide the data array (contains all the parameters to run prediction) and directory path. \n",
        "#Returns the filepath of the PDB structure of the variant\n",
        "def run_prediction(jobname,data_array,directory_name):\n",
        "  result_dir=\".\"\n",
        "  if 'logging_setup' not in globals():\n",
        "      setup_logging(Path(\".\").joinpath(\"log.txt\"))\n",
        "      logging_setup = True\n",
        "  \n",
        "  queries_path=data_array[0]\n",
        "  use_amber = data_array[1]\n",
        "  template_mode = data_array[2]\n",
        "  custom_template_path = data_array[3]\n",
        "  use_templates = data_array[4]\n",
        "  msa_mode = data_array[5]\n",
        "  pair_mode = data_array[6]\n",
        "  a3m_file = data_array[7]\n",
        "  model_type = data_array[8]\n",
        "  num_recycles = data_array[9]\n",
        "  save_to_google_drive = data_array[10]\n",
        "  dpi = data_array[11]\n",
        "  \n",
        "  queries, is_complex = get_queries(queries_path)\n",
        "  model_type = set_model_type(is_complex, model_type)\n",
        "  download_alphafold_params(model_type, Path(\".\"))\n",
        "  run(\n",
        "      queries=queries,\n",
        "      result_dir=result_dir,\n",
        "      use_templates=use_templates,\n",
        "      custom_template_path=custom_template_path,\n",
        "      use_amber=use_amber,\n",
        "      msa_mode=msa_mode,    \n",
        "      model_type=model_type,\n",
        "      num_models=5,\n",
        "      num_recycles=num_recycles,\n",
        "      model_order=[1, 2, 3, 4, 5],\n",
        "      is_complex=is_complex,\n",
        "      data_dir=Path(\".\"),\n",
        "      keep_existing_results=False,\n",
        "      recompile_padding=1.0,\n",
        "      rank_by=\"auto\",\n",
        "      pair_mode=pair_mode,\n",
        "      stop_at_score=float(100),\n",
        "      prediction_callback=prediction_callback,\n",
        "      dpi=dpi\n",
        "  )\n",
        "\n",
        "  # Moves rank 1 model # files to PDBs folder\n",
        "  ex_end_path = \"\"\n",
        "  for index in range(1,6):\n",
        "    end_file = \"_unrelaxed_rank_1_model_\"+str(index)+\".pdb\"\n",
        "    test_path = \"/content/\" + jobname + end_file\n",
        "    if (os.path.exists(test_path)):\n",
        "      ex_end_path = end_file\n",
        "\n",
        "  old_path = \"/content/\" + jobname + ex_end_path\n",
        "  new_path = \"/content/\"+ directory_name +\"/\" + jobname + ex_end_path\n",
        "  os.replace(old_path,new_path)\n",
        "\n",
        "  return new_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rCk6EPmgFPoW"
      },
      "outputs": [],
      "source": [
        "#@markdown ### 4. Run RMSD Functionality\n",
        "#@markdown aligned PDB structures of variants stored in \"aligned\" directory.\n",
        "#@markdown Directory then has structures corresponding for each protein. \n",
        "\n",
        "#Obtains the tags that were selected by the user\n",
        "def process_tags(variant_dict):\n",
        "  keys = []\n",
        "  variant_dict.keys()\n",
        "  for key in variant_dict.keys():\n",
        "    if not key[9:] in keys:\n",
        "      keys.append(key[9:])\n",
        "  return keys\n",
        "\n",
        "#Takes the given dictionaries for both natives and variant sequences, and performs RMSD calculations\n",
        "def obtain_RMSD(native_dict,variant_dict):\n",
        "  data = process_tags(variant_dict)\n",
        "  for dir in os.listdir(\"/content/aligned/\"):\n",
        "    path = \"/content/aligned/\" + dir\n",
        "    shutil.rmtree(path)\n",
        "\n",
        "  native_names = native_dict.keys()\n",
        "  variant_names = variant_dict.keys()\n",
        "  rmsd_dfs=[]\n",
        "  for native_name in native_names:\n",
        "    parser = PDB.PDBParser()\n",
        "    ref_structure = parser.get_structure(native_name, native_dict[native_name])\n",
        "\n",
        "    model = ref_structure[0]\n",
        "    res_no= 0\n",
        "    non_resi = 0\n",
        "\n",
        "    for model in ref_structure:\n",
        "        for chain in model:\n",
        "            for r in chain.get_residues():\n",
        "                if r.id[0] == ' ':\n",
        "                    res_no +=1\n",
        "                else:\n",
        "                    non_resi +=1\n",
        "\n",
        "    res_no2 = 0\n",
        "    non_resi2 = 0\n",
        "    for model in ref_structure:\n",
        "      for residue in model.get_residues():\n",
        "          if PDB.is_aa(residue):\n",
        "              res_no2 += 1\n",
        "          else:\n",
        "              non_resi2 += 1\n",
        "\n",
        "    res_count = res_no\n",
        "\n",
        "    start_id = 1\n",
        "    end_id = res_no\n",
        "    atoms_to_be_aligned = range(start_id,end_id+1)\n",
        "\n",
        "    align_dir = \"aligned/\" + native_name[:8]\n",
        "    os.mkdir(align_dir)\n",
        "\n",
        "    label_index = 0\n",
        "    values = []\n",
        "    for variant_name in variant_names:\n",
        "      if (variant_name[:8] == native_name[:8]):\n",
        "        ref_structure = parser.get_structure(variant_name, variant_dict[variant_name])\n",
        "        sample_structure = parser.get_structure(native_name, native_dict[native_name])\n",
        "      \n",
        "        ref_model    = ref_structure[0]\n",
        "        sample_model = sample_structure[0]\n",
        "\n",
        "        ref_atoms = []\n",
        "        sample_atoms = []\n",
        "\n",
        "         # Iterate of all chains in the model in order to find all residues\n",
        "        for ref_chain in ref_model:\n",
        "          # Iterate of all residues in each model in order to find proper atoms\n",
        "          for ref_res in ref_chain:\n",
        "            # Check if residue number ( .get_id() ) is in the list\n",
        "            if ref_res.get_id()[1] in atoms_to_be_aligned:\n",
        "              # Append CA atom to list\n",
        "              ref_atoms.append(ref_res['CA'])\n",
        "        \n",
        "        for sample_chain in sample_model:\n",
        "          for sample_res in sample_chain:\n",
        "            if sample_res.get_id()[1] in atoms_to_be_aligned:\n",
        "              sample_atoms.append(sample_res['CA'])\n",
        "\n",
        "        super_imposer = Bio.PDB.Superimposer()\n",
        "        super_imposer.set_atoms(ref_atoms, sample_atoms)\n",
        "        super_imposer.apply(sample_model.get_atoms())\n",
        "        values.append(super_imposer.rms)\n",
        "\n",
        "        io = Bio.PDB.PDBIO()\n",
        "        io.set_structure(sample_structure)\n",
        "        filename = native_name+\"_\"+data[label_index] + \"_aligned_.pdb\"\n",
        "        io.save(filename)\n",
        "        os.path.join(align_dir+\"/\",filename)\n",
        "        old_path = \"/content/\" + filename\n",
        "        new_path = \"/content/aligned/\" + native_name[:8] + \"/\" + filename\n",
        "        os.replace(old_path,new_path)\n",
        "        \n",
        "        label_index += 1\n",
        "      else:\n",
        "        continue\n",
        "    rmsd_dfs.append(values)\n",
        "\n",
        "  N = len(rmsd_dfs)\n",
        "  fig = plt.figure()\n",
        "  if(N == 1):\n",
        "    plt.bar(data,rmsd_dfs[0], width = 0.4,label = \"Protein 0\")\n",
        "    plt.xlabel(\"Tags\")\n",
        "    plt.ylabel(\"RMSD (Angstroms)\")\n",
        "    plt.title(\"RMSD calculations for tagged vs native sequence\")\n",
        "    plt.legend()\n",
        "  else:\n",
        "    multi_rmsd = []\n",
        "    column_data = [\"Tag\"]\n",
        "    for tag_index in range (0,len(data)):\n",
        "      tag_data = [data[tag_index]]\n",
        "      for protein in rmsd_dfs:\n",
        "        tag_data.append(protein[tag_index])\n",
        "      multi_rmsd.append(tag_data)\n",
        "      column_data.append(\"Protein \"+str(tag_index))\n",
        "\n",
        "    df = pd.DataFrame((multi_rmsd),columns=column_data)\n",
        "    figure = df.plot(x='Tag',kind='bar',stacked=False,rot=\"0\",ylabel=\"RMSD values\",title=\"RMSD calculations for tagged vs native sequence\")\n",
        "\n",
        "  return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "mEC2_ISr41Ze",
        "outputId": "58813930-7025-4f7d-a401-e19ba5e7a764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "Running on public URL: https://25215.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://25215.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<gradio.routes.App at 0x7f6f7955cd90>,\n",
              " 'http://127.0.0.1:7863/',\n",
              " 'https://25215.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#@title Render user interface\n",
        "\n",
        "# Interface to determine number of proteins to compare where n is the number of proteins\n",
        "# 1 <= n <= 10\n",
        "\n",
        "files = {}\n",
        "\n",
        "#Function that is called to update input textbox for sequences\n",
        "def count_protein(quantity):\n",
        "    if (quantity == 1):\n",
        "        return gr.Textbox.update(lines=1, visible=True,placeholder=\"Enter protein sequence.\")\n",
        "    else:\n",
        "        return gr.Textbox.update(lines=quantity, visible=True,placeholder=\"Enter protein sequences separated by a semicolon\\nEnter a new sequence per line.\")\n",
        "\n",
        "#Function that is called at user submission to run ColabFold iteratively, calculate RMSD, and create a plot\n",
        "def parse_choices(protein_num,seq_string,tag_choices):\n",
        "    filepaths_list = process_variants(process_str(seq_string),tag_choices)\n",
        "    native_list = input_dict(filepaths_list[0],True)\n",
        "    variant_list = input_dict(filepaths_list[1],False)\n",
        "    files = [native_list,variant_list]\n",
        "    plot = obtain_RMSD(native_list,variant_list)\n",
        "    return plot\n",
        "\n",
        "# Breaks down the input sequence(s)\n",
        "def process_str(seq_string):\n",
        "    seq_list = seq_string.split(\";\\n\")\n",
        "    return seq_list\n",
        "\n",
        "#obtains the filepaths of native and variant sequences and stores them as separate arrays, returns an array that contains the two.\n",
        "def process_variants(seq_list,tag_choices):\n",
        "    variant_list = {}\n",
        "    native_list = {}\n",
        "    for seq_index in range (0,len(seq_list)):\n",
        "        ori_seq = \"Protein\" + str(seq_index)\n",
        "        native_list[ori_seq] = seq_list[seq_index]\n",
        "        for tag in tag_choices:\n",
        "            variant_name = ori_seq + \"_\" + tag\n",
        "            variant_seq = seq_list[seq_index] + tag_sequences[tag]\n",
        "            variant_list[variant_name] = variant_seq\n",
        "    filepaths = [native_list,variant_list]\n",
        "    return filepaths\n",
        "        \n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\n",
        "    \"\"\"\n",
        "    # Uni-Tagger\n",
        "    Utilized ColabFold to predict structures of tag variant proteins and compare them to the native conformational state.\n",
        "    Important:\n",
        "    - Be sure to ONLY input the number of protein sequences as you indicate on the slider.\n",
        "    - Split the protein sequences with a semicolon appended to the previous sequence. Then on a newline, enter the next protein structure. \n",
        "    - For the last protein structure, DO NOT append a semicolon\n",
        "    - This calculation is intesnive, may need ColabPro or might need to run on local machine for better results.\n",
        "    - Warning: Selecting mNEONGreen may lead to longer runtimes, which may increase chances of failure to complete data production\n",
        "    - If you would like to use the link made by gradio.app, open in a separate tab but ensure that THIS notebook instance remains open\n",
        "    Configure below:\n",
        "    \"\"\")\n",
        "    slider = gr.Slider(1,10,1,step=1,label=\"Number of Protein(s)\")\n",
        "    text = gr.Textbox(lines=1, interactive=True,placeholder=\"Enter protein sequence.\",label=\"Protein Sequence(s)\")\n",
        "    checkgroup = gr.CheckboxGroup([\"6X His\",\"CBP\",\"CBD + 3G\",\"mNEONGreen + 5G\"],label=\"Select TAGs\")\n",
        "    generate = gr.Button()\n",
        "    with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                rmsd_plot = gr.Plot()\n",
        "    \n",
        "    slider.change(fn=count_protein,inputs=slider,outputs=text)\n",
        "    generate.click(fn=parse_choices,inputs=[slider,text,checkgroup],outputs=rmsd_plot)\n",
        "\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}